{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7938c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a580cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    data = pd.read_csv('resumesdataset.csv', encoding='utf-8')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path.\")\n",
    "    raise\n",
    "\n",
    "# Inspect data\n",
    "print(\"Data overview:\")\n",
    "print(data.head())\n",
    "print(\"\\nData info:\")\n",
    "print(data.info())\n",
    "print(\"\\nData description:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nDuplicate records:\")\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "# Remove duplicates\n",
    "data.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.xticks(rotation=90)\n",
    "ax=sns.countplot(x=\"Category\", data=resumeDataSet)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba8633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "targetCounts = resumeDataSet['Category'].value_counts()\n",
    "targetLabels  = resumeDataSet['Category'].unique()\n",
    "# Make square figures and axes\n",
    "plt.figure(1, figsize=(22,22))\n",
    "the_grid = GridSpec(2, 2)\n",
    "\n",
    "\n",
    "cmap = plt.get_cmap('coolwarm')\n",
    "plt.subplot(the_grid[0, 1], aspect=1, title='CATEGORY DISTRIBUTION')\n",
    "\n",
    "source_pie = plt.pie(targetCounts, labels=targetLabels, autopct='%1.1f%%', shadow=True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)       # Remove numbers\n",
    "    text = re.sub(r'\\W+', ' ', text)      # Remove special characters\n",
    "    text = text.lower()                   # Convert to lowercase\n",
    "    words = word_tokenize(text)           # Tokenize the text\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply cleaning to the \"Resume\" column\n",
    "data['cleaned_resume'] = data['Resume'].apply(clean_text)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "oneSetOfStopWords = set(stopwords.words('english')+['``',\"''\"])\n",
    "totalWords =[]\n",
    "Sentences = resumeDataSet['Resume'].values\n",
    "cleanedSentences = \"\"\n",
    "for records in Sentences:\n",
    "    cleanedText = cleanResume(records)\n",
    "    cleanedSentences += cleanedText\n",
    "    requiredWords = nltk.word_tokenize(cleanedText)\n",
    "    for word in requiredWords:\n",
    "        if word not in oneSetOfStopWords and word not in string.punctuation:\n",
    "            totalWords.append(word)\n",
    "    \n",
    "wordfreqdist = nltk.FreqDist(totalWords)\n",
    "mostcommon = wordfreqdist.most_common(50)\n",
    "print(mostcommon)\n",
    "\n",
    "wc = WordCloud().generate(cleanedSentences)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(data['cleaned_resume'])\n",
    "\n",
    "# 'X' now contains the feature vectors\n",
    "y = data['Category']  # This will be the target variable for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202fa83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64131f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'models/model.pkl')\n",
    "joblib.dump(vectorizer, 'models/vectorizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c498b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f965f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
